{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch as t\n",
    "import numpy as np\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import plotly.graph_objects as go \n",
    "from functools import partial\n",
    "import tqdm.auto as tqdm\n",
    "import circuitsvis as cv\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "from transformer_lens.components import Embed, Unembed, LayerNorm, MLP\n",
    "from fancy_einsum import einsum\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import re\n",
    "\n",
    "#from plotly_utils import imshow, line, scatter, bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting functions \n",
    "update_layout_set = {\n",
    "    \"xaxis_range\", \"yaxis_range\", \"hovermode\", \"xaxis_title\", \"yaxis_title\", \"colorbar\", \"colorscale\", \"coloraxis\", \"title_x\", \"bargap\", \"bargroupgap\", \"xaxis_tickformat\",\n",
    "    \"yaxis_tickformat\", \"title_y\", \"legend_title_text\", \"xaxis_showgrid\", \"xaxis_gridwidth\", \"xaxis_gridcolor\", \"yaxis_showgrid\", \"yaxis_gridwidth\", \"yaxis_gridcolor\",\n",
    "    \"showlegend\", \"xaxis_tickmode\", \"yaxis_tickmode\", \"xaxis_tickangle\", \"yaxis_tickangle\", \"margin\", \"xaxis_visible\", \"yaxis_visible\", \"bargap\", \"bargroupgap\"\n",
    "}\n",
    "\n",
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## turn off AD to save memory, since we're focusing on model inference here \n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate example prompts for IOI along with clean and corrupted answers. It's important that they're all the same length (taken from exploratory analysis demo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_format = [\n",
    "    \"When John and Mary went to the shops,{} gave the bag to\",\n",
    "    \"When Tom and James went to the park,{} gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops,{} gave an apple to\",\n",
    "    \"After Martin and Amy went to the park,{} gave a drink to\",\n",
    "]\n",
    "\n",
    "names = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "# List of prompts\n",
    "prompts = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers = []\n",
    "# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)\n",
    "answer_tokens = []\n",
    "for i in range(len(prompt_format)):\n",
    "    for j in range(2):\n",
    "        answers.append((names[i][j], names[i][1 - j]))\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                model.to_single_token(answers[-1][0]),\n",
    "                model.to_single_token(answers[-1][1]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        prompts.append(prompt_format[i].format(answers[-1][1]))\n",
    "answer_tokens = t.tensor(answer_tokens).to(device)\n",
    "\n",
    "### check that all the prompts have the same number of tokens \n",
    "prompt_len = len(model.to_str_tokens(prompts[1]))\n",
    "assert len(set([len(model.to_str_tokens(prompt)) for prompt in prompts])) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  prompts and answers                                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> prompt                                                        </span>┃<span style=\"font-weight: bold\"> clean   </span>┃<span style=\"font-weight: bold\"> corrupted </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave the bag to    │  Mary   │  John     │\n",
       "│ When John and Mary went to the shops, Mary gave the bag to    │  John   │  Mary     │\n",
       "│ When Tom and James went to the park, James gave the ball to   │  Tom    │  James    │\n",
       "│ When Tom and James went to the park, Tom gave the ball to     │  James  │  Tom      │\n",
       "│ When Dan and Sid went to the shops, Sid gave an apple to      │  Dan    │  Sid      │\n",
       "│ When Dan and Sid went to the shops, Dan gave an apple to      │  Sid    │  Dan      │\n",
       "│ After Martin and Amy went to the park, Amy gave a drink to    │  Martin │  Amy      │\n",
       "│ After Martin and Amy went to the park, Martin gave a drink to │  Amy    │  Martin   │\n",
       "└───────────────────────────────────────────────────────────────┴─────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  prompts and answers                                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mprompt                                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mclean  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrupted\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave the bag to    │  Mary   │  John     │\n",
       "│ When John and Mary went to the shops, Mary gave the bag to    │  John   │  Mary     │\n",
       "│ When Tom and James went to the park, James gave the ball to   │  Tom    │  James    │\n",
       "│ When Tom and James went to the park, Tom gave the ball to     │  James  │  Tom      │\n",
       "│ When Dan and Sid went to the shops, Sid gave an apple to      │  Dan    │  Sid      │\n",
       "│ When Dan and Sid went to the shops, Dan gave an apple to      │  Sid    │  Dan      │\n",
       "│ After Martin and Amy went to the park, Amy gave a drink to    │  Martin │  Amy      │\n",
       "│ After Martin and Amy went to the park, Martin gave a drink to │  Amy    │  Martin   │\n",
       "└───────────────────────────────────────────────────────────────┴─────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### print all prompts in a table (learned from Keith's notebook! )\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "prompt_tab = Table('prompt', 'clean', 'corrupted', title = 'prompts and answers')\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    prompt_tab.add_row(prompts[i], answers[i][0], answers[i][1])\n",
    "\n",
    "rprint(prompt_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache the logits and model internals for all the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ex = len(prompts)\n",
    "\n",
    "\n",
    "tokens = model.to_tokens(prompts, prepend_bos = True).to(device)\n",
    "og_logits, cache = model.run_with_cache(tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a metric to test model performance. In this case, we'll use the logit difference between the indirect object (correct answer) and the subject (incorrect answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                 Logit differences                                                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Prompt                                          </span>┃<span style=\"font-weight: bold\"> Correct </span>┃<span style=\"font-weight: bold\"> Incorrect </span>┃<span style=\"font-weight: bold\"> Logit Difference </span>┃<span style=\"font-weight: bold\"> Avg Logit Difference </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Mary   </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  John     </span>│<span style=\"font-weight: bold\"> 3.337            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the bag to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When John and Mary went to the shops, Mary gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  John   </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Mary     </span>│<span style=\"font-weight: bold\"> 3.202            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the bag to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Tom and James went to the park, James gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Tom    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  James    </span>│<span style=\"font-weight: bold\"> 2.709            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the ball to                                     │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Tom and James went to the park, Tom gave   │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  James  </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Tom      </span>│<span style=\"font-weight: bold\"> 3.797            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the ball to                                     │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Dan and Sid went to the shops, Sid gave an │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Dan    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Sid      </span>│<span style=\"font-weight: bold\"> 1.720            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ apple to                                        │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Dan and Sid went to the shops, Dan gave an │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Sid    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Dan      </span>│<span style=\"font-weight: bold\"> 5.281            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ apple to                                        │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ After Martin and Amy went to the park, Amy gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Martin </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Amy      </span>│<span style=\"font-weight: bold\"> 2.601            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ a drink to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ After Martin and Amy went to the park, Martin   │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Amy    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Martin   </span>│<span style=\"font-weight: bold\"> 5.767            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ gave a drink to                                 │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "└─────────────────────────────────────────────────┴─────────┴───────────┴──────────────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                 Logit differences                                                 \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIncorrect\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLogit Difference\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAvg Logit Difference\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Mary  \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m John    \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.337           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the bag to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When John and Mary went to the shops, Mary gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m John  \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Mary    \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.202           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the bag to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Tom and James went to the park, James gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Tom   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m James   \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2.709           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the ball to                                     │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Tom and James went to the park, Tom gave   │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m James \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Tom     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.797           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the ball to                                     │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Dan and Sid went to the shops, Sid gave an │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Dan   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Sid     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m1.720           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ apple to                                        │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Dan and Sid went to the shops, Dan gave an │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Sid   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Dan     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m5.281           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ apple to                                        │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ After Martin and Amy went to the park, Amy gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Martin\u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Amy     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2.601           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ a drink to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ After Martin and Amy went to the park, Martin   │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Amy   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Martin  \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m5.767           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ gave a drink to                                 │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "└─────────────────────────────────────────────────┴─────────┴───────────┴──────────────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_vocab = model.cfg.d_vocab\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "n_layers = model.cfg.n_layers\n",
    "\n",
    "\n",
    "\n",
    "assert og_logits.shape == t.Size([n_ex, prompt_len, d_vocab])\n",
    "\n",
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt = False):\n",
    "    # take the last logit for every prompt (only these are relevant to the answer)\n",
    "    final_logits = logits[:,-1,:]\n",
    "    # get the logits corresponding to the IO/ sub tokens \n",
    "    answer_logits = final_logits.gather(dim=-1, index = answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:,0] - answer_logits[:,1]\n",
    "    ## If per_prompt = True, return an array of the per_prompt difference, instead of the average \n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "    \n",
    "\n",
    "og_logit_diff = logits_to_ave_logit_diff(og_logits, answer_tokens, per_prompt=True)\n",
    "og_logit_avg_diff = logits_to_ave_logit_diff(og_logits, answer_tokens, per_prompt=False)\n",
    "\n",
    "cols = [\n",
    "    \"Prompt\", \n",
    "    Column(\"Correct\", style=\"rgb(0,200,0) bold\"), \n",
    "    Column(\"Incorrect\", style=\"rgb(255,0,0) bold\"), \n",
    "    Column(\"Logit Difference\", style=\"bold\"), Column(\"Avg Logit Difference\", style=\"bold\")\n",
    "]\n",
    "logit_diff_table = Table(*cols, title=\"Logit differences\")\n",
    "\n",
    "for prompt, ans, logit_diff in zip(prompts, answers,og_logit_diff):\n",
    "    logit_diff_table.add_row(prompt, ans[0], ans[1], f\"{logit_diff.item():.3f}\")\n",
    "rprint(logit_diff_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5335,  1757],\n",
       "        [ 1757,  5335],\n",
       "        [ 4186,  3700],\n",
       "        [ 3700,  4186],\n",
       "        [ 6035, 15686],\n",
       "        [15686,  6035],\n",
       "        [ 5780, 14235],\n",
       "        [14235,  5780]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean string 0:     <|endoftext|>When John and Mary went to the shops, John gave the bag to \n",
      "Corrupted string 0: <|endoftext|>When John and Mary went to the shops, Mary gave the bag to\n",
      "Clean logit diff: 3.5519\n",
      "Corrupted logit diff: -3.5519\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import patching\n",
    "\n",
    "clean_tokens = tokens \n",
    "idx_swap = [i+1 if i % 2 == 0 else i-1 for i in range(len(tokens))]\n",
    "corrupted_tokens = clean_tokens[idx_swap]\n",
    "\n",
    "print(\n",
    "    \"Clean string 0:    \", model.to_string(clean_tokens[0]), \"\\n\"\n",
    "    \"Corrupted string 0:\", model.to_string(corrupted_tokens[0])\n",
    ")\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = logits_to_ave_logit_diff(clean_logits, answer_tokens)\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_metric(\n",
    "    logits: Float[t.Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Float[t.Tensor, \"batch 2\"] = answer_tokens,\n",
    "    corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    ") -> Float[t.Tensor, \"\"]:\n",
    "\n",
    "    patched_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)\n",
    "    return (patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff  - corrupted_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Baseline is 1: 1.0000\n",
      "Corrupted Baseline is 0: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# checking that this does what we want \n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_lin_comb_cache(clean_cache, corrupted_cache, lh_list):\n",
    "    aux_cache = copy.deepcopy(clean_cache)\n",
    "    clean_comb = 0\n",
    "    corrupted_comb = 0\n",
    "    count = 0\n",
    "\n",
    "    for layer, head in lh_list:\n",
    "        count += 1\n",
    "        clean_comb += clean_cache[utils.get_act_name(\"z\", layer)][:,:,head,:]\n",
    "        corrupted_comb += corrupted_cache[utils.get_act_name(\"z\", layer)][:,:,head,:]\n",
    "        #print(f\"1:{layer}\")\n",
    "    comb = (clean_comb - corrupted_comb)/count\n",
    "    for layer, head in lh_list:\n",
    "\n",
    "        #print(f\"2:{layer,head}\")\n",
    "        aux_cache[utils.get_act_name(\"z\", layer)][:,:,head,:] += comb\n",
    "\n",
    "    return aux_cache\n",
    "\n",
    "lh_list_pos = [[9,6],[9,9], [10,0]]\n",
    "lh_list_neg = [[10,7], [11,10]]\n",
    "lh_list_tot = lh_list_neg + lh_list_pos\n",
    "\n",
    "pos_cache = get_lin_comb_cache(clean_cache, corrupted_cache, lh_list_pos)\n",
    "neg_cache = get_lin_comb_cache(clean_cache, corrupted_cache, lh_list_neg)\n",
    "total_cache = get_lin_comb_cache(clean_cache, corrupted_cache, lh_list_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Optional, Callable, Tuple, Dict, Literal, Set\n",
    "\n",
    "\n",
    "def patch_head_vector(\n",
    "    corrupted_head_vector: Float[t.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint,\n",
    "    head_index: int,\n",
    "    from_cache: ActivationCache\n",
    ") -> Float[t.Tensor, \"batch pos head_index d_head\"]:\n",
    "    '''\n",
    "    Patches the output of a given head (before it's added to the residual stream) at\n",
    "    every sequence position, using the value from the aux cache (either clean or linear comb).\n",
    "    '''\n",
    "    corrupted_head_vector[:, :, head_index] = from_cache[hook.name][:, :, head_index]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "def get_act_patched(\n",
    "    model: HookedTransformer,\n",
    "    corrupted_tokens: Float[t.Tensor, \"batch pos\"],\n",
    "    from_cache: ActivationCache,\n",
    "    patching_metric: Callable,\n",
    "    lh_list\n",
    ") -> Float[t.Tensor, \"layer head\"]:\n",
    "    '''\n",
    "    Returns an array of results of patching at all positions for each head in each\n",
    "    layer, using the value from the clean cache.\n",
    "\n",
    "    The results are calculated using the patching_metric function, which should be\n",
    "    called on the model's logit output.\n",
    "    '''\n",
    "    model.reset_hooks()\n",
    "    result = t.zeros(model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=t.float32)\n",
    "\n",
    "    head_hooks = [(utils.get_act_name(\"z\", layer),partial(patch_head_vector, head_index=head, from_cache=from_cache)) for layer, head in lh_list]\n",
    "\n",
    "\n",
    "    patched_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks = head_hooks,\n",
    "            return_type=\"logits\"\n",
    "        )\n",
    "    result = patching_metric(patched_logits)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len([(utils.get_act_name(\"z\", layer),partial(patch_head_vector, head_index=head, from_cache=clean_cache)) for layer, head in lh_list_neg]), len(lh_list_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Heads: result(lin comb) = 0.6618, result(individual) = 0.3353\n",
      "- Heads: result(lin comb) = -1.2503, result(individual) = -0.8568\n",
      "All Heads: result(lin comb) = 0.3069, result(individual) = 0.1690\n",
      "Fraction recovered by positive heads:1.9737\n",
      "Fraction recovered by negative heads:-1.4592\n",
      "Fraction recovered by all heads:1.8162\n"
     ]
    }
   ],
   "source": [
    "act_patch_attn_indiv_pos = get_act_patched(model, corrupted_tokens, clean_cache, ioi_metric, lh_list_pos)\n",
    "act_patch_attn_lin_comb_pos = get_act_patched(model, corrupted_tokens, pos_cache, ioi_metric, lh_list_pos)\n",
    "\n",
    "act_patch_attn_indiv_neg = get_act_patched(model, corrupted_tokens, clean_cache, ioi_metric, lh_list_neg)\n",
    "act_patch_attn_lin_comb_neg = get_act_patched(model, corrupted_tokens, neg_cache, ioi_metric, lh_list_neg)\n",
    "\n",
    "act_patch_attn_indiv_tot = get_act_patched(model, corrupted_tokens, clean_cache, ioi_metric, lh_list_tot)\n",
    "act_patch_attn_lin_comb_tot = get_act_patched(model, corrupted_tokens, total_cache, ioi_metric, lh_list_tot)\n",
    "\n",
    "print(f'+ Heads: result(lin comb) = {act_patch_attn_lin_comb_pos:.4f}, result(individual) = {act_patch_attn_indiv_pos:.4f}')\n",
    "print(f'- Heads: result(lin comb) = {act_patch_attn_lin_comb_neg:.4f}, result(individual) = {act_patch_attn_indiv_neg:.4f}')\n",
    "print(f'All Heads: result(lin comb) = {act_patch_attn_lin_comb_tot:.4f}, result(individual) = {act_patch_attn_indiv_tot:.4f}')\n",
    "\n",
    "print(f'Fraction recovered by positive heads:{(act_patch_attn_lin_comb_pos)/t.abs(act_patch_attn_indiv_pos):.4f}')\n",
    "print(f'Fraction recovered by negative heads:{(act_patch_attn_lin_comb_neg)/t.abs(act_patch_attn_indiv_neg):.4f}')\n",
    "print(f'Fraction recovered by all heads:{(act_patch_attn_lin_comb_tot)/t.abs(act_patch_attn_indiv_tot):.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in backup name movers to see if it makes a difference: First compare individually patched heads + backups with linear combination of just +/- movers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Heads: result(lin comb) = 0.6618, result(individual+ backup) = 0.6764\n",
      "- Heads: result(lin comb) = -1.2503, result(individual+backup) = -0.6919\n",
      "All Heads: result(lin comb) = 0.3069, result(individual+backup) = 0.6992\n",
      "Fraction recovered by positive heads:0.9783\n",
      "Fraction recovered by negative heads:-1.8069\n",
      "Fraction recovered by all heads:0.4389\n"
     ]
    }
   ],
   "source": [
    "lh_backup = [[10,10],[10,6],[10,2],[10,1],[11,2],[11,9],[9,0],[9,7]]\n",
    "\n",
    "lh_neg_bu = lh_list_neg + lh_backup\n",
    "lh_pos_bu = lh_list_pos + lh_backup\n",
    "lh_tot_bu = lh_list_tot + lh_backup\n",
    "\n",
    "act_patch_attn_indiv_pos_bu = get_act_patched(model, corrupted_tokens, clean_cache, ioi_metric, lh_pos_bu)\n",
    "act_patch_attn_indiv_neg_bu = get_act_patched(model, corrupted_tokens, clean_cache, ioi_metric, lh_neg_bu)\n",
    "act_patch_attn_indiv_tot_bu = get_act_patched(model, corrupted_tokens, clean_cache, ioi_metric, lh_tot_bu)\n",
    "\n",
    "print(f'+ Heads: result(lin comb) = {act_patch_attn_lin_comb_pos:.4f}, result(individual+ backup) = {act_patch_attn_indiv_pos_bu:.4f}')\n",
    "print(f'- Heads: result(lin comb) = {act_patch_attn_lin_comb_neg:.4f}, result(individual+backup) = {act_patch_attn_indiv_neg_bu:.4f}')\n",
    "print(f'All Heads: result(lin comb) = {act_patch_attn_lin_comb_tot:.4f}, result(individual+backup) = {act_patch_attn_indiv_tot_bu:.4f}')\n",
    "\n",
    "print(f'Fraction recovered by positive heads:{(act_patch_attn_lin_comb_pos)/t.abs(act_patch_attn_indiv_pos_bu):.4f}')\n",
    "print(f'Fraction recovered by negative heads:{(act_patch_attn_lin_comb_neg)/t.abs(act_patch_attn_indiv_neg_bu):.4f}')\n",
    "print(f'Fraction recovered by all heads:{(act_patch_attn_lin_comb_tot)/t.abs(act_patch_attn_indiv_tot_bu):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Heads: result(lin comb + bu) = 0.7651, result(individual + bu) = 0.6764\n",
      "- Heads: result(lin comb + bu) = -0.7013, result(individual + bu) = -0.6919\n",
      "All Heads: result(lin comb + bu) = 0.8322, result(individual + bu) = 0.6992\n",
      "Fraction recovered by positive heads:1.1310\n",
      "Fraction recovered by negative heads:-1.0135\n",
      "Fraction recovered by all heads:1.1903\n"
     ]
    }
   ],
   "source": [
    "### Now patch in the linear combination of heads + backup heads, and compare with above \n",
    "\n",
    "pos_cache_bu = get_lin_comb_cache(clean_cache, corrupted_cache, lh_pos_bu)\n",
    "neg_cache_bu = get_lin_comb_cache(clean_cache, corrupted_cache, lh_neg_bu)\n",
    "total_cache_bu = get_lin_comb_cache(clean_cache, corrupted_cache, lh_tot_bu)\n",
    "\n",
    "act_patch_attn_lin_comb_pos_bu = get_act_patched(model, corrupted_tokens, pos_cache_bu, ioi_metric, lh_pos_bu)\n",
    "act_patch_attn_lin_comb_neg_bu = get_act_patched(model, corrupted_tokens, pos_cache_bu, ioi_metric, lh_neg_bu)\n",
    "act_patch_attn_lin_comb_tot_bu = get_act_patched(model, corrupted_tokens, pos_cache_bu, ioi_metric, lh_tot_bu)\n",
    "\n",
    "print(f'+ Heads: result(lin comb + bu) = {act_patch_attn_lin_comb_pos_bu:.4f}, result(individual + bu) = {act_patch_attn_indiv_pos_bu:.4f}')\n",
    "print(f'- Heads: result(lin comb + bu) = {act_patch_attn_lin_comb_neg_bu:.4f}, result(individual + bu) = {act_patch_attn_indiv_neg_bu:.4f}')\n",
    "print(f'All Heads: result(lin comb + bu) = {act_patch_attn_lin_comb_tot_bu:.4f}, result(individual + bu) = {act_patch_attn_indiv_tot_bu:.4f}')\n",
    "\n",
    "print(f'Fraction recovered by positive heads:{(act_patch_attn_lin_comb_pos_bu)/t.abs(act_patch_attn_indiv_pos_bu):.4f}')\n",
    "print(f'Fraction recovered by negative heads:{(act_patch_attn_lin_comb_neg_bu)/t.abs(act_patch_attn_indiv_neg_bu):.4f}')\n",
    "print(f'Fraction recovered by all heads:{(act_patch_attn_lin_comb_tot_bu)/t.abs(act_patch_attn_indiv_tot_bu):.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above was fed through the rest of the network, but we want to study the direct logit attribution instead. In this case, we treat each head as if they're in the same layer (equivalent to changing the basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5335,  1757],\n",
       "        [ 1757,  5335],\n",
       "        [ 4186,  3700],\n",
       "        [ 3700,  4186],\n",
       "        [ 6035, 15686],\n",
       "        [15686,  6035],\n",
       "        [ 5780, 14235],\n",
       "        [14235,  5780]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.14662, 55.2978)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_head_out(input_cache, layer, head):      \n",
    "    attn_out = einops.einsum(model.W_O[layer,head], input_cache[utils.get_act_name(\"z\", layer)][:,:,head],\"d_h d_m, n s d_h -> n s d_m\") + model.b_O[layer]\n",
    "    return einops.einsum(model.W_U, attn_out, \"d_m n_ctx, b s d_m -> b s n_ctx \")\n",
    "\n",
    "\n",
    "def compare_patched_logits(indiv_cache, lin_comb_cache, lh_list, answer_tokens):\n",
    "    indiv_logits = [logits_to_ave_logit_diff(get_head_out(indiv_cache, layer, head), answer_tokens) for layer, head in lh_list]\n",
    "    lin_logits = [logits_to_ave_logit_diff(get_head_out(lin_comb_cache, layer, head), answer_tokens) for layer, head in lh_list]\n",
    "    return np.sum(indiv_logits), np.sum(lin_logits)\n",
    "\n",
    "compare_patched_logits(clean_cache, total_cache, lh_list_tot, answer_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tracing the negative name mover (last one) through the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_head = model.cfg.d_head\n",
    "assert model.W_O.shape == t.Size([n_layers,n_heads,d_head,d_model])\n",
    "assert cache[utils.get_act_name(\"z\", 11)].shape == t.Size([n_ex,prompt_len,n_heads,d_head])\n",
    "\n",
    "# act with W_O\n",
    "last_neg_out_no_bias = einops.einsum(model.W_O[11,10], cache[utils.get_act_name(\"z\", 11)][:,:,10],\"d_h d_m, n s d_h -> n s d_m\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still to do: path patching and debug. Make sure the direct logit attribution for the uniform linear comb is the same as the individual patch "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e851e82dedee55c73100bfbe6682b3cabddc188516229e1e8f6342cda6eae37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
