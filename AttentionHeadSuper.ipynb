{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch as t\n",
    "import numpy as np\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import plotly.graph_objects as go \n",
    "from functools import partial\n",
    "import tqdm.auto as tqdm\n",
    "import circuitsvis as cv\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "from transformer_lens.components import Embed, Unembed, LayerNorm, MLP\n",
    "from fancy_einsum import einsum\n",
    "#from plotly_utils import imshow, line, scatter, bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## turn off AD to save memory, since we're focusing on model inference here \n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate example prompts for IOI along with clean and corrupted answers. It's important that they're all the same length (taken from exploratory analysis demo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = [\n",
    "    \"When John and Mary went to the shops,{} gave the bag to\",\n",
    "    \"When Tom and James went to the park,{} gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops,{} gave an apple to\",\n",
    "    \"After Martin and Amy went to the park,{} gave a drink to\",\n",
    "]\n",
    "names = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "# List of prompts\n",
    "prompts = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers = []\n",
    "# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)\n",
    "answer_tokens = []\n",
    "for i in range(len(prompt_format)):\n",
    "    for j in range(2):\n",
    "        answers.append((names[i][j], names[i][1 - j]))\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                model.to_single_token(answers[-1][0]),\n",
    "                model.to_single_token(answers[-1][1]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        prompts.append(prompt_format[i].format(answers[-1][1]))\n",
    "answer_tokens = t.tensor(answer_tokens).to(device)\n",
    "\n",
    "### check that all the prompts have the same number of tokens \n",
    "prompt_len = len(model.to_str_tokens(prompts[1]))\n",
    "assert len(set([len(model.to_str_tokens(prompt)) for prompt in prompts])) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  prompts and answers                                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> prompt                                                        </span>┃<span style=\"font-weight: bold\"> clean   </span>┃<span style=\"font-weight: bold\"> corrupted </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave the bag to    │  Mary   │  John     │\n",
       "│ When John and Mary went to the shops, Mary gave the bag to    │  John   │  Mary     │\n",
       "│ When Tom and James went to the park, James gave the ball to   │  Tom    │  James    │\n",
       "│ When Tom and James went to the park, Tom gave the ball to     │  James  │  Tom      │\n",
       "│ When Dan and Sid went to the shops, Sid gave an apple to      │  Dan    │  Sid      │\n",
       "│ When Dan and Sid went to the shops, Dan gave an apple to      │  Sid    │  Dan      │\n",
       "│ After Martin and Amy went to the park, Amy gave a drink to    │  Martin │  Amy      │\n",
       "│ After Martin and Amy went to the park, Martin gave a drink to │  Amy    │  Martin   │\n",
       "└───────────────────────────────────────────────────────────────┴─────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  prompts and answers                                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mprompt                                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mclean  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrupted\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave the bag to    │  Mary   │  John     │\n",
       "│ When John and Mary went to the shops, Mary gave the bag to    │  John   │  Mary     │\n",
       "│ When Tom and James went to the park, James gave the ball to   │  Tom    │  James    │\n",
       "│ When Tom and James went to the park, Tom gave the ball to     │  James  │  Tom      │\n",
       "│ When Dan and Sid went to the shops, Sid gave an apple to      │  Dan    │  Sid      │\n",
       "│ When Dan and Sid went to the shops, Dan gave an apple to      │  Sid    │  Dan      │\n",
       "│ After Martin and Amy went to the park, Amy gave a drink to    │  Martin │  Amy      │\n",
       "│ After Martin and Amy went to the park, Martin gave a drink to │  Amy    │  Martin   │\n",
       "└───────────────────────────────────────────────────────────────┴─────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### print all prompts in a table (learned from Keith's notebook! )\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "prompt_tab = Table('prompt', 'clean', 'corrupted', title = 'prompts and answers')\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    prompt_tab.add_row(prompts[i], answers[i][0], answers[i][1])\n",
    "\n",
    "rprint(prompt_tab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache the logits and model internals for all the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompts, prepend_bos = True).to(device)\n",
    "og_logits, cache = model.run_with_cache(tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a metric to test model performance. In this case, we'll use the logit difference between the indirect object (correct answer) and the subject (incorrect answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                 Logit differences                                                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Prompt                                          </span>┃<span style=\"font-weight: bold\"> Correct </span>┃<span style=\"font-weight: bold\"> Incorrect </span>┃<span style=\"font-weight: bold\"> Logit Difference </span>┃<span style=\"font-weight: bold\"> Avg Logit Difference </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Mary   </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  John     </span>│<span style=\"font-weight: bold\"> 3.337            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the bag to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When John and Mary went to the shops, Mary gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  John   </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Mary     </span>│<span style=\"font-weight: bold\"> 3.202            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the bag to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Tom and James went to the park, James gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Tom    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  James    </span>│<span style=\"font-weight: bold\"> 2.709            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the ball to                                     │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Tom and James went to the park, Tom gave   │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  James  </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Tom      </span>│<span style=\"font-weight: bold\"> 3.797            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the ball to                                     │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Dan and Sid went to the shops, Sid gave an │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Dan    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Sid      </span>│<span style=\"font-weight: bold\"> 1.720            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ apple to                                        │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Dan and Sid went to the shops, Dan gave an │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Sid    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Dan      </span>│<span style=\"font-weight: bold\"> 5.281            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ apple to                                        │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ After Martin and Amy went to the park, Amy gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Martin </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Amy      </span>│<span style=\"font-weight: bold\"> 2.601            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ a drink to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ After Martin and Amy went to the park, Martin   │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Amy    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Martin   </span>│<span style=\"font-weight: bold\"> 5.767            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ gave a drink to                                 │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "└─────────────────────────────────────────────────┴─────────┴───────────┴──────────────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                 Logit differences                                                 \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIncorrect\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLogit Difference\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAvg Logit Difference\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Mary  \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m John    \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.337           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the bag to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When John and Mary went to the shops, Mary gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m John  \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Mary    \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.202           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the bag to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Tom and James went to the park, James gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Tom   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m James   \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2.709           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the ball to                                     │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Tom and James went to the park, Tom gave   │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m James \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Tom     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.797           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the ball to                                     │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Dan and Sid went to the shops, Sid gave an │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Dan   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Sid     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m1.720           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ apple to                                        │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Dan and Sid went to the shops, Dan gave an │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Sid   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Dan     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m5.281           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ apple to                                        │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ After Martin and Amy went to the park, Amy gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Martin\u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Amy     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2.601           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ a drink to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ After Martin and Amy went to the park, Martin   │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Amy   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Martin  \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m5.767           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ gave a drink to                                 │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "└─────────────────────────────────────────────────┴─────────┴───────────┴──────────────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_vocab = model.cfg.d_vocab\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "n_layers = model.cfg.n_layers\n",
    "n_ex = len(prompts)\n",
    "\n",
    "assert og_logits.shape == t.Size([n_ex, prompt_len, d_vocab])\n",
    "\n",
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt = False):\n",
    "    # take the last logit for every prompt (only these are relevant to the answer)\n",
    "    final_logits = logits[:,-1,:]\n",
    "    # get the logits corresponding to the IO/ sub tokens \n",
    "    answer_logits = final_logits.gather(dim=-1, index = answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:,0] - answer_logits[:,1]\n",
    "    ## If per_prompt = True, return an array of the per_prompt difference, instead of the average \n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "    \n",
    "\n",
    "og_logit_diff = logits_to_ave_logit_diff(og_logits, answer_tokens, per_prompt=True)\n",
    "og_logit_avg_diff = logits_to_ave_logit_diff(og_logits, answer_tokens, per_prompt=False)\n",
    "\n",
    "cols = [\n",
    "    \"Prompt\", \n",
    "    Column(\"Correct\", style=\"rgb(0,200,0) bold\"), \n",
    "    Column(\"Incorrect\", style=\"rgb(255,0,0) bold\"), \n",
    "    Column(\"Logit Difference\", style=\"bold\"), Column(\"Avg Logit Difference\", style=\"bold\")\n",
    "]\n",
    "logit_diff_table = Table(*cols, title=\"Logit differences\")\n",
    "\n",
    "for prompt, ans, logit_diff in zip(prompts, answers,og_logit_diff):\n",
    "    logit_diff_table.add_row(prompt, ans[0], ans[1], f\"{logit_diff.item():.3f}\")\n",
    "rprint(logit_diff_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5519)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_logit_avg_diff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on in IOI? There are several ways to check. \n",
    "\n",
    "First, Direct Logit Attribution: The residual stream is read and written to with linear maps (+ a ~linear (?) layernorm), so its logits can be decomposed into the sum from each linear function acting on it. Working backwards from the end of the model (logits = U(LN(final_residual))), see which components contribute most to the logit for the right token. \n",
    "\n",
    "- The metric for IOI is nice, since the difference of the log probabilities (log softmax) is the same as the difference for the logits\n",
    "\n",
    "- Getting an output logit = projecting onto the residual stream in that direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 768])\n",
      "tensor(-1.1921e-06)\n"
     ]
    }
   ],
   "source": [
    "# map answer tokens to the d_model residual stream directions\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(answer_residual_directions.shape)\n",
    "logit_diff_directions = answer_residual_directions[:,0] - answer_residual_directions[:,1]\n",
    "\n",
    "## Make sure this works by applying U and LN to the residual stream \n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type]. \n",
    "\n",
    "final_residual = cache[\"resid_post\", -1]\n",
    "assert final_residual.shape == t.Size([n_ex, prompt_len, d_model])\n",
    "\n",
    "final_tok_residual = final_residual[:,-1,:]\n",
    "\n",
    "# divide by the LN scale. pos_slice are the positions considered (final token of each prompt)\n",
    "scaled_final_token_residual = cache.apply_ln_to_stack(final_tok_residual, layer = -1, pos_slice=-1)\n",
    "\n",
    "## Get the average logit diff by projecting onto the answer residual stream directions and summing over each direction and example\n",
    "average_avg_logit_diff = (einsum(\"b d_model, b d_model -> \", scaled_final_token_residual, logit_diff_directions).item())/n_ex\n",
    "\n",
    "# small enough! \n",
    "print(average_avg_logit_diff - og_logit_avg_diff)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e851e82dedee55c73100bfbe6682b3cabddc188516229e1e8f6342cda6eae37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
