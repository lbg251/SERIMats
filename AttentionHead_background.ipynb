{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch as t\n",
    "import numpy as np\n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "import plotly.express as px\n",
    "import einops\n",
    "import plotly.graph_objects as go \n",
    "from functools import partial\n",
    "import tqdm.auto as tqdm\n",
    "import circuitsvis as cv\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "from transformer_lens.components import Embed, Unembed, LayerNorm, MLP\n",
    "from fancy_einsum import einsum\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import re\n",
    "\n",
    "#from plotly_utils import imshow, line, scatter, bar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting functions \n",
    "update_layout_set = {\n",
    "    \"xaxis_range\", \"yaxis_range\", \"hovermode\", \"xaxis_title\", \"yaxis_title\", \"colorbar\", \"colorscale\", \"coloraxis\", \"title_x\", \"bargap\", \"bargroupgap\", \"xaxis_tickformat\",\n",
    "    \"yaxis_tickformat\", \"title_y\", \"legend_title_text\", \"xaxis_showgrid\", \"xaxis_gridwidth\", \"xaxis_gridcolor\", \"yaxis_showgrid\", \"yaxis_gridwidth\", \"yaxis_gridcolor\",\n",
    "    \"showlegend\", \"xaxis_tickmode\", \"yaxis_tickmode\", \"xaxis_tickangle\", \"yaxis_tickangle\", \"margin\", \"xaxis_visible\", \"yaxis_visible\", \"bargap\", \"bargroupgap\"\n",
    "}\n",
    "\n",
    "def imshow(tensor, renderer=None, **kwargs):\n",
    "    kwargs_post = {k: v for k, v in kwargs.items() if k in update_layout_set}\n",
    "    kwargs_pre = {k: v for k, v in kwargs.items() if k not in update_layout_set}\n",
    "    facet_labels = kwargs_pre.pop(\"facet_labels\", None)\n",
    "    border = kwargs_pre.pop(\"border\", False)\n",
    "    if \"color_continuous_scale\" not in kwargs_pre:\n",
    "        kwargs_pre[\"color_continuous_scale\"] = \"RdBu\"\n",
    "    if \"margin\" in kwargs_post and isinstance(kwargs_post[\"margin\"], int):\n",
    "        kwargs_post[\"margin\"] = dict.fromkeys(list(\"tblr\"), kwargs_post[\"margin\"])\n",
    "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, **kwargs_pre)\n",
    "    if facet_labels:\n",
    "        for i, label in enumerate(facet_labels):\n",
    "            fig.layout.annotations[i]['text'] = label\n",
    "    if border:\n",
    "        fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "        fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
    "    # things like `xaxis_tickmode` should be applied to all subplots. This is super janky lol but I'm under time pressure\n",
    "    for setting in [\"tickangle\"]:\n",
    "      if f\"xaxis_{setting}\" in kwargs_post:\n",
    "          i = 2\n",
    "          while f\"xaxis{i}\" in fig[\"layout\"]:\n",
    "            kwargs_post[f\"xaxis{i}_{setting}\"] = kwargs_post[f\"xaxis_{setting}\"]\n",
    "            i += 1\n",
    "    fig.update_layout(**kwargs_post)\n",
    "    fig.show(renderer=renderer)\n",
    "\n",
    "def line(tensor, renderer=None, **kwargs):\n",
    "    px.line(y=utils.to_numpy(tensor), **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "## turn off AD to save memory, since we're focusing on model inference here \n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
    "model = HookedTransformer.from_pretrained('gpt2-small', device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate example prompts for IOI along with clean and corrupted answers. It's important that they're all the same length (taken from exploratory analysis demo )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = [\n",
    "    \"When John and Mary went to the shops,{} gave the bag to\",\n",
    "    \"When Tom and James went to the park,{} gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops,{} gave an apple to\",\n",
    "    \"After Martin and Amy went to the park,{} gave a drink to\",\n",
    "]\n",
    "names = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "# List of prompts\n",
    "prompts = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers = []\n",
    "# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)\n",
    "answer_tokens = []\n",
    "for i in range(len(prompt_format)):\n",
    "    for j in range(2):\n",
    "        answers.append((names[i][j], names[i][1 - j]))\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                model.to_single_token(answers[-1][0]),\n",
    "                model.to_single_token(answers[-1][1]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        prompts.append(prompt_format[i].format(answers[-1][1]))\n",
    "answer_tokens = t.tensor(answer_tokens).to(device)\n",
    "\n",
    "### check that all the prompts have the same number of tokens \n",
    "prompt_len = len(model.to_str_tokens(prompts[1]))\n",
    "assert len(set([len(model.to_str_tokens(prompt)) for prompt in prompts])) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                  prompts and answers                                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> prompt                                                        </span>┃<span style=\"font-weight: bold\"> clean   </span>┃<span style=\"font-weight: bold\"> corrupted </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave the bag to    │  Mary   │  John     │\n",
       "│ When John and Mary went to the shops, Mary gave the bag to    │  John   │  Mary     │\n",
       "│ When Tom and James went to the park, James gave the ball to   │  Tom    │  James    │\n",
       "│ When Tom and James went to the park, Tom gave the ball to     │  James  │  Tom      │\n",
       "│ When Dan and Sid went to the shops, Sid gave an apple to      │  Dan    │  Sid      │\n",
       "│ When Dan and Sid went to the shops, Dan gave an apple to      │  Sid    │  Dan      │\n",
       "│ After Martin and Amy went to the park, Amy gave a drink to    │  Martin │  Amy      │\n",
       "│ After Martin and Amy went to the park, Martin gave a drink to │  Amy    │  Martin   │\n",
       "└───────────────────────────────────────────────────────────────┴─────────┴───────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                  prompts and answers                                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mprompt                                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mclean  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcorrupted\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave the bag to    │  Mary   │  John     │\n",
       "│ When John and Mary went to the shops, Mary gave the bag to    │  John   │  Mary     │\n",
       "│ When Tom and James went to the park, James gave the ball to   │  Tom    │  James    │\n",
       "│ When Tom and James went to the park, Tom gave the ball to     │  James  │  Tom      │\n",
       "│ When Dan and Sid went to the shops, Sid gave an apple to      │  Dan    │  Sid      │\n",
       "│ When Dan and Sid went to the shops, Dan gave an apple to      │  Sid    │  Dan      │\n",
       "│ After Martin and Amy went to the park, Amy gave a drink to    │  Martin │  Amy      │\n",
       "│ After Martin and Amy went to the park, Martin gave a drink to │  Amy    │  Martin   │\n",
       "└───────────────────────────────────────────────────────────────┴─────────┴───────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### print all prompts in a table (learned from Keith's notebook! )\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "prompt_tab = Table('prompt', 'clean', 'corrupted', title = 'prompts and answers')\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    prompt_tab.add_row(prompts[i], answers[i][0], answers[i][1])\n",
    "\n",
    "rprint(prompt_tab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache the logits and model internals for all the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompts, prepend_bos = True).to(device)\n",
    "og_logits, cache = model.run_with_cache(tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a metric to test model performance. In this case, we'll use the logit difference between the indirect object (correct answer) and the subject (incorrect answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                 Logit differences                                                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Prompt                                          </span>┃<span style=\"font-weight: bold\"> Correct </span>┃<span style=\"font-weight: bold\"> Incorrect </span>┃<span style=\"font-weight: bold\"> Logit Difference </span>┃<span style=\"font-weight: bold\"> Avg Logit Difference </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Mary   </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  John     </span>│<span style=\"font-weight: bold\"> 3.337            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the bag to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When John and Mary went to the shops, Mary gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  John   </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Mary     </span>│<span style=\"font-weight: bold\"> 3.202            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the bag to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Tom and James went to the park, James gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Tom    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  James    </span>│<span style=\"font-weight: bold\"> 2.709            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the ball to                                     │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Tom and James went to the park, Tom gave   │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  James  </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Tom      </span>│<span style=\"font-weight: bold\"> 3.797            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ the ball to                                     │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Dan and Sid went to the shops, Sid gave an │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Dan    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Sid      </span>│<span style=\"font-weight: bold\"> 1.720            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ apple to                                        │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ When Dan and Sid went to the shops, Dan gave an │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Sid    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Dan      </span>│<span style=\"font-weight: bold\"> 5.281            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ apple to                                        │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ After Martin and Amy went to the park, Amy gave │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Martin </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Amy      </span>│<span style=\"font-weight: bold\"> 2.601            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ a drink to                                      │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ After Martin and Amy went to the park, Martin   │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">  Amy    </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">  Martin   </span>│<span style=\"font-weight: bold\"> 5.767            </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "│ gave a drink to                                 │<span style=\"color: #00c800; text-decoration-color: #00c800; font-weight: bold\">         </span>│<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">           </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">                      </span>│\n",
       "└─────────────────────────────────────────────────┴─────────┴───────────┴──────────────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                 Logit differences                                                 \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPrompt                                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrect\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mIncorrect\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mLogit Difference\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAvg Logit Difference\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ When John and Mary went to the shops, John gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Mary  \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m John    \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.337           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the bag to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When John and Mary went to the shops, Mary gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m John  \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Mary    \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.202           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the bag to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Tom and James went to the park, James gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Tom   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m James   \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2.709           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the ball to                                     │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Tom and James went to the park, Tom gave   │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m James \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Tom     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m3.797           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ the ball to                                     │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Dan and Sid went to the shops, Sid gave an │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Dan   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Sid     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m1.720           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ apple to                                        │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ When Dan and Sid went to the shops, Dan gave an │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Sid   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Dan     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m5.281           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ apple to                                        │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ After Martin and Amy went to the park, Amy gave │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Martin\u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Amy     \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m2.601           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ a drink to                                      │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "│ After Martin and Amy went to the park, Martin   │\u001b[1;38;2;0;200;0m \u001b[0m\u001b[1;38;2;0;200;0m Amy   \u001b[0m\u001b[1;38;2;0;200;0m \u001b[0m│\u001b[1;38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0m Martin  \u001b[0m\u001b[1;38;2;255;0;0m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m5.767           \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                    \u001b[0m\u001b[1m \u001b[0m│\n",
       "│ gave a drink to                                 │\u001b[1;38;2;0;200;0m         \u001b[0m│\u001b[1;38;2;255;0;0m           \u001b[0m│\u001b[1m                  \u001b[0m│\u001b[1m                      \u001b[0m│\n",
       "└─────────────────────────────────────────────────┴─────────┴───────────┴──────────────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_vocab = model.cfg.d_vocab\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "n_layers = model.cfg.n_layers\n",
    "n_ex = len(prompts)\n",
    "\n",
    "assert og_logits.shape == t.Size([n_ex, prompt_len, d_vocab])\n",
    "\n",
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt = False):\n",
    "    # take the last logit for every prompt (only these are relevant to the answer)\n",
    "    final_logits = logits[:,-1,:]\n",
    "    # get the logits corresponding to the IO/ sub tokens \n",
    "    answer_logits = final_logits.gather(dim=-1, index = answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:,0] - answer_logits[:,1]\n",
    "    ## If per_prompt = True, return an array of the per_prompt difference, instead of the average \n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "    \n",
    "\n",
    "og_logit_diff = logits_to_ave_logit_diff(og_logits, answer_tokens, per_prompt=True)\n",
    "og_logit_avg_diff = logits_to_ave_logit_diff(og_logits, answer_tokens, per_prompt=False)\n",
    "\n",
    "cols = [\n",
    "    \"Prompt\", \n",
    "    Column(\"Correct\", style=\"rgb(0,200,0) bold\"), \n",
    "    Column(\"Incorrect\", style=\"rgb(255,0,0) bold\"), \n",
    "    Column(\"Logit Difference\", style=\"bold\"), Column(\"Avg Logit Difference\", style=\"bold\")\n",
    "]\n",
    "logit_diff_table = Table(*cols, title=\"Logit differences\")\n",
    "\n",
    "for prompt, ans, logit_diff in zip(prompts, answers,og_logit_diff):\n",
    "    logit_diff_table.add_row(prompt, ans[0], ans[1], f\"{logit_diff.item():.3f}\")\n",
    "rprint(logit_diff_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5519)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_logit_avg_diff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on in IOI? There are several ways to check. \n",
    "\n",
    "First, Direct Logit Attribution: The residual stream is read and written to with linear maps (+ a ~linear (?) layernorm), so its logits can be decomposed into the sum from each linear function acting on it. Working backwards from the end of the model (logits = U(LN(final_residual))), see which components contribute most to the logit for the right token. \n",
    "\n",
    "- The metric for IOI is nice, since the difference of the log probabilities (log softmax) is the same as the difference for the logits\n",
    "\n",
    "- Getting an output logit = projecting onto the residual stream in that direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2, 768])\n",
      "tensor(-1.1921e-06)\n"
     ]
    }
   ],
   "source": [
    "# map answer tokens to the d_model residual stream directions\n",
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(answer_residual_directions.shape)\n",
    "logit_diff_directions = answer_residual_directions[:,0] - answer_residual_directions[:,1]\n",
    "\n",
    "## Make sure this works by applying U and LN to the residual stream \n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type]. \n",
    "\n",
    "final_residual = cache[\"resid_post\", -1]\n",
    "assert final_residual.shape == t.Size([n_ex, prompt_len, d_model])\n",
    "\n",
    "final_tok_residual = final_residual[:,-1,:]\n",
    "\n",
    "# divide by the LN scale. pos_slice are the positions considered (final token of each prompt)\n",
    "scaled_final_token_residual = cache.apply_ln_to_stack(final_tok_residual, layer = -1, pos_slice=-1)\n",
    "\n",
    "## Get the average logit diff by projecting onto the answer residual stream directions and summing over each direction and example\n",
    "average_avg_logit_diff = (einsum(\"b d_model, b d_model -> \", scaled_final_token_residual, logit_diff_directions).item())/n_ex\n",
    "\n",
    "# small enough! \n",
    "print(average_avg_logit_diff - og_logit_avg_diff)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit Lens: Decompose the residual stream using the logit difference method at every layer. This tracks the accumulated affects of the attention and MLP at each layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"669f41a9-9306-4a58-9950-c1a21f157b6b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"669f41a9-9306-4a58-9950-c1a21f157b6b\")) {                    Plotly.newPlot(                        \"669f41a9-9306-4a58-9950-c1a21f157b6b\",                        [{\"hovertemplate\":\"<b>%{hovertext}</b><br><br>x=%{x}<br>y=%{y}<extra></extra>\",\"hovertext\":[\"0_pre\",\"0_mid\",\"1_pre\",\"1_mid\",\"2_pre\",\"2_mid\",\"3_pre\",\"3_mid\",\"4_pre\",\"4_mid\",\"5_pre\",\"5_mid\",\"6_pre\",\"6_mid\",\"7_pre\",\"7_mid\",\"8_pre\",\"8_mid\",\"9_pre\",\"9_mid\",\"10_pre\",\"10_mid\",\"11_pre\",\"11_mid\",\"final_post\"],\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0,5.5,6.0,6.5,7.0,7.5,8.0,8.5,9.0,9.5,10.0,10.5,11.0,11.5,12.0],\"xaxis\":\"x\",\"y\":[1.294112007599324e-05,-0.006643432192504406,-0.007525029592216015,-0.00907558761537075,-0.008736645802855492,-0.008685395121574402,-0.006480309180915356,-0.007939750328660011,-0.009662089869379997,-0.01509622298181057,-0.014190541580319405,-0.01992950402200222,-0.009124262258410454,-0.027297884225845337,-0.02985457330942154,0.24972766637802124,0.25056034326553345,0.45005473494529724,0.45997166633605957,5.025465965270996,5.14291524887085,4.730584144592285,4.887079238891602,3.4454002380371094,3.5518875122070312],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Difference From Accumulate Residual Stream\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('669f41a9-9306-4a58-9950-c1a21f157b6b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## general function from above: residual stack is a stack of residual stream components at each part of the network you want to look at \n",
    "def resid_stack_to_logit_diff(resid_stack: Float[t.Tensor, 'components b d_model'], cache:ActivationCache)-> float:\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(resid_stack, layer = -1, pos_slice=-1)\n",
    "    return (einsum(\"... b d_model, b d_model -> ...\", scaled_residual_stack, logit_diff_directions))/n_ex\n",
    "\n",
    "## get the residuals and labels from the cache\n",
    "accumulated_residual, labels = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n",
    "logit_lens_logit_diffs = resid_stack_to_logit_diff(accumulated_residual, cache)\n",
    "\n",
    "\n",
    "line(logit_lens_logit_diffs, x = np.arange(2*n_layers+1)/2,hover_name=labels, title=\"Logit Difference From Accumulate Residual Stream\")\n",
    "\n",
    "### The early layers (through 7) cannot accomplish the IOI task. After that, performance improves, with the biggest contribution coming from the attention layer in the 9th block (9_mid). Layers after 9 mainly decrease performance.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Attribution: Repeat the above, but treating each layer separately. This is like looking at the differences in adjacent residual streams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>                            <div id=\"c08e2552-28c5-42a4-930f-629a4596b946\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c08e2552-28c5-42a4-930f-629a4596b946\")) {                    Plotly.newPlot(                        \"c08e2552-28c5-42a4-930f-629a4596b946\",                        [{\"hovertemplate\":\"<b>%{hovertext}</b><br><br>x=%{x}<br>y=%{y}<extra></extra>\",\"hovertext\":[\"embed\",\"pos_embed\",\"0_attn_out\",\"0_mlp_out\",\"1_attn_out\",\"1_mlp_out\",\"2_attn_out\",\"2_mlp_out\",\"3_attn_out\",\"3_mlp_out\",\"4_attn_out\",\"4_mlp_out\",\"5_attn_out\",\"5_mlp_out\",\"6_attn_out\",\"6_mlp_out\",\"7_attn_out\",\"7_mlp_out\",\"8_attn_out\",\"8_mlp_out\",\"9_attn_out\",\"9_mlp_out\",\"10_attn_out\",\"10_mlp_out\",\"11_attn_out\",\"11_mlp_out\"],\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25],\"xaxis\":\"x\",\"y\":[-0.00028366869082674384,0.0002966101747006178,-0.006656370125710964,-0.000881607411429286,-0.0015505594201385975,0.0003389469056855887,5.125184543430805e-05,0.002205091994255781,-0.0014594392850995064,-0.0017223481554538012,-0.005434119142591953,0.0009056851267814636,-0.005738968029618263,0.01080525666475296,-0.018173644319176674,-0.0025566909462213516,0.27958226203918457,0.0008327127434313297,0.19949433207511902,0.009917007759213448,4.565494537353516,0.11744954437017441,-0.41233134269714355,0.15649539232254028,-1.4416792392730713,0.10648781061172485],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Difference From Each Layer\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c08e2552-28c5-42a4-930f-629a4596b946');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_layer_logit_diffs = resid_stack_to_logit_diff(per_layer_residual, cache)\n",
    "\n",
    "\n",
    "line(per_layer_logit_diffs,hover_name=labels, title=\"Logit Difference From Each Layer\")\n",
    "\n",
    "### The attention layers matter much more than the MLPs, meaning this task values information transfer between tokens more than processing that information. Again, layers after 9 decrease performance \n",
    "#### This is a good test to run at the start of each new experiment! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head Attribution: Which attention heads matter most?\n",
    "Attention heads are independent and  - analyze the contributions of each. \n",
    "\n",
    "- Positive name mover heads: 10.0, 9.6, 9.9. These have a positive logit diff\n",
    "- Negative name mover heads: 10.7, 11.10. These have a negative logit diff -- they incentivize adding to the wrong name\n",
    "\n",
    "Doing a full analysis of the attention patterns per head would show that both of these attend to the indirect object, (copying the names decided on by the QK circuit with the OV circuit). \n",
    "\n",
    "The attention patterns only look at the end, which feeds directly into the logits.\n",
    "\n",
    "Note: Be careful about confusing the tokens themselves with their positions in the residual stream! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Patching! \n",
    "\n",
    "1. Run the model on two inputs (clean and corrupted prompts, defined above). For the denoising scheme described here, start with the corrupted prompt, which outputs the incorrect answer. \n",
    "\n",
    "2. Intervene on a specific activation by patching in the activation from the clean prompt. \n",
    "\n",
    "3. See how much this nudges the output toward the correct answer. Do this for many activations. Which ones significantly increase the probability of the correct answer? This causally traces the output to its activations, allowing us to reserse engineer a circuit for a given behavior. However, it does not tell us exactly how these pieces interact. \n",
    "\n",
    "TO DO: Compare this method (corrupted -> clean) to its reverse (clean -> corrupted). Naively: \n",
    "- clean -> corrupted (noising) starts with a circuit meant to achieve a task. If you add in \"wrong\" information and that doesn't change performance, it means that piece wasn't necessary. \n",
    "- corrupted -> clean (denoising) starts with an incorrect circuit. Adding information bit by bit, when performance stops performing, you have the sufficient circuit. Wouldn't detect backup name mover heads!?\n",
    "\n",
    "Normalize the logit diff so we can see how much performance is improved without referring back to the original. Does >1 performance come from averaging over many examples, or is it saying something real about the circuit? (Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise)\n",
    "    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance\n",
    "\n",
    "Takeaways from itnervening on various parts of the network: \n",
    "\n",
    "- residual stream: patching in clean residual stream for all layers pretty much recovers performance exactly.  \n",
    "- attention out: attention matters most on early layers for the second subject token, and later layers for the final token (some positive, some negative), and are very localized. \n",
    "- MLP_out: MLP layers don't seem to matter, except for layer 0. Neel guesses that this is because of the tied embedding? \n",
    "- All heads individually (heads have dim (head_index, position, layer)): Patching the activation z (weighted attention score) \n",
    "- Further patch the attention patterns (QK circuit - where to move info. finding: early and late layers matter more) or value vetors (OV circuit - what info to move. finding: middle layers matter more).We probably don't need to do this -- what other information could it add? \n",
    "\n",
    "\n",
    "Question: IOI apparently have induction heads at the beginning of the network. Since these detect and copy repeated sequences instead of repeated words (names), this is probably repurposed technology from pretraining. The model doesn't learn a new type of \"copy token\" head. \n",
    "Two follow-up questions here: \n",
    "\n",
    "1. Are s-inhibition heads general properties of all transformers? (maybe this has been studied?)\n",
    "\n",
    "2. Could something similar be happening for name mover heads? That is, is there a more general head that could accomplish this task and more? This may be simple, like proper-noun or capital word detection, or more complicated. Perhaps the +/- name mover heads are really the same type of more general head, with a different meaning. This may give us more intuition for superposition (or replace it?). Actually, it's easier to study the heads in smaller circuits and then build them up... \n",
    "\n",
    "Sanity check Experiment: how do these act off distribution? (to other names , proper nouns? They probably need a preposition. Perhals they are prepositional heads?)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting to it: Attention head superposition!\n",
    "\n",
    "Overall questions: \n",
    "1. Superposition as an idea makese sense, and seems likely. But for a given specific case, given that attention head \"features\" are ill-defined, how can we tell if it's really happening? Is there a change of basis for the z activations that would put all of these heads in the same layer? \n",
    "2. What are the weights of superposition in IOI? \n",
    "3. Are backup name mover heads important here? If they recover some performance of the desired task, couldn't they take over here too if the negative name mover heads are too high to complete it? \n",
    "4. Are these heads a more specialized application of another head \"type\" ubiquitous in all language models, and does that matter for superposition? \n",
    "5. What is the relationship between superposition and polysemanticity for attention heads? (Clearer for neurons)\n",
    "6. What is an attention head feature? \n",
    "7. Is a metric of 1 the best we can do? (Can we acheive better than original performance by patching linear combination of weights?)\n",
    "\n",
    "TODO: \n",
    "- perform different kinds of patching (activation, attribution, path) to compare its  to the per-head patched \n",
    "- test the uniform combination and one weighted by each head's attention paid to the IO, on average (Neel's idea), and maybe come up with one more\n",
    "- learn the real gradients by gradient descent (L1 loss for sparsity). Does Atticus Geiger's method of GD and Causal Scrubbing help? \n",
    "- Come up with a toy implementation in 1 or 2 layer attention-only models a la Anthropic\n",
    "- Test on out of distribution data (max activating examples)\n",
    "- Try to reason about the role of non-linearities between the layers (if the only dimension that matters for superposition is the head index, maybe this is fine.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e851e82dedee55c73100bfbe6682b3cabddc188516229e1e8f6342cda6eae37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
